Stochastic Variational Bayes
============================

Stochastic Variational Bayes is a method of performing Bayesian inference on the parameters
of a generative model given a data set assumed to be generated by the model with 
unknown additive noise.

Bayesian inference
------------------

Bayes' theorem is a general statement about how ones belief about the value distribution
of a random variable should be updated in the light of new data. In the context of
model parameter inference it can be described as follows:

:math:`P(M \mid D) = \frac{P(D \mid M) \, P(M)}{P(D)}`

Here :math:`P(M \mid D)` is the *posterior* distribution, i.e. the distribution of the
inferred model parameters M given the data D.

:math:`P(D \mid M)` is the *likelihood*, i.e. the probability of getting the data D
from a given set of model parameters M. This is determined by evaluating the model
prediction using the parameters M and comparing it to the data. The difference between
the two must be the result of noise, and the likelihood of the noise can be calculated
from the noise model.

:math:`P(M)` is the *prior* probability of the model parameters M. This describes the
distribution we would believe the parameters would follow before any data has been seen
and might reflect, for example, existing estimates of physiological parameters or other
constraints (e.g. that a fractional parameter must lie between 0 and 1).

:math:`P(D)` is the *evidence* and is chiefly used when comparing one model with another.
For an inference problem using a single model it can be neglected as it is independent
of the parameters.

Variational Bayes
-----------------

The general Bayesian inference problem can, in general, only be solved by a sampling
method such as Markov Chain Monte Carlo (MCMC) where random samples are generated in
such a way that, through Bayes' theorem, they gradually provide a representative 
sample of the posterior distribution.

MCMC, however, is extremely computationally intensive especially for the kind of 
applications we are concerned with where we may be fitting between 2 and 20 parameters
independently at typically :math:`10^5` voxels. Variational Bayes is an approximate
method which re-formulates the inference problem in the form of a variational
principle, i.e. one where we seek to maximise the *Free Energy*.

The advantage of a variational approach is that simplified forms can be chosen for the
prior and posterior such that the free energy can be calculated and optimized in a 
computationally efficient manner. Typically we assume multivariate Gaussian 
distributions for the prior and posterior, and a noise model based on a Gamma 
distribution.

One form of variational Bayes uses the calculus of variations to derive a set of
update equations for the model and noise parameters which can then be iterated 
until convergence. However this method requires particular choices of the prior
and posterior distributions, and the noise model, and thus lacks flexibility.

Stochastic variational Bayes
----------------------------

The stochastic method is based on evaluating the free energy during optimization
by taking a sample from the current posterior distribution. As the free energy
is optimized, the posterior becomes closer to the 'correct' posterior distribution.
The method is more flexible because the only requirements on the prior and posterior
are the ability to take samples and calculate basic likelihood functions. Maximisation
of the free energy can then be done using a generic framework, such as those
developed for machine learning applications.

